---
---
@inproceedings{wang-etal-2021-learning-similarity,
    title = "Learning Similarity between Movie Characters and Its Potential Implications on Understanding Human Experiences",
    author = "Wang, Zhilin  and
      Lin, Weizhe  and
      Wu, Xiaodong",
    booktitle = "Proceedings of the Third Workshop on Narrative Understanding",
    month = jun,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nuse-1.3",
    pdf = "https://aclanthology.org/2021.nuse-1.3",
    doi = "10.18653/v1/2021.nuse-1.3",
    pages = "24--35",
    abstract = "While many different aspects of human experiences have been studied by the NLP community, none has captured its full richness. We propose a new task to capture this richness based on an unlikely setting: movie characters. We sought to capture theme-level similarities between movie characters that were community-curated into 20,000 themes. By introducing a two-step approach that balances performance and efficiency, we managed to achieve 9-27{\%} improvement over recent paragraph-embedding based methods. Finally, we demonstrate how the thematic information learnt from movie characters can potentially be used to understand themes in the experience of people, as indicated on Reddit posts.",
}

@inproceedings{wang-etal-2019-youre,
    title = "No, you{'}re not alone: A better way to find people with similar experiences on {R}eddit",
    author = "Wang, Zhilin  and
      Rastorgueva, Elena  and
      Lin, Weizhe  and
      Wu, Xiaodong",
    booktitle = "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5540",
    pdf = "https://aclanthology.org/D19-5540",
    doi = "10.18653/v1/D19-5540",
    pages = "307--315",
    abstract = "We present a probabilistic clustering algorithm that can help Reddit users to find posts that discuss experiences similar to their own. This model is built upon the BERT Next Sentence Prediction model and reduces the time complexity for clustering all posts in a corpus from O(n{\^{}}2) to O(n) with respect to the number of posts. We demonstrate that such probabilistic clustering can yield a performance better than baseline clustering methods based on Latent Dirichlet Allocation (Blei et al., 2003) and Word2Vec (Mikolov et al., 2013). Furthermore, there is a high degree of coherence between our probabilistic clustering and the exhaustive comparison O(n{\^{}}2) algorithm in which the similarity between every pair of posts is found. This makes the use of the BERT Next Sentence Prediction model more practical for unsupervised clustering tasks due to the high runtime overhead of each BERT computation.",
}
