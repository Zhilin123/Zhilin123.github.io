---
---
@inproceedings{wang-etal-2021-learning-similarity,
    title = "Learning Similarity between Movie Characters and Its Potential Implications on Understanding Human Experiences",
    author = "Wang, Zhilin  and
      Lin, Weizhe  and
      Wu, Xiaodong",
    booktitle = "Proceedings of the Third Workshop on Narrative Understanding @ NAACL",
    month = jun,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nuse-1.3",
    pdf = "https://aclanthology.org/2021.nuse-1.3",
    doi = "10.18653/v1/2021.nuse-1.3",
    pages = "24--35",
    abstract = "While many different aspects of human experiences have been studied by the NLP community, none has captured its full richness. We propose a new task to capture this richness based on an unlikely setting: movie characters. We sought to capture theme-level similarities between movie characters that were community-curated into 20,000 themes. By introducing a two-step approach that balances performance and efficiency, we managed to achieve 9-27{\%} improvement over recent paragraph-embedding based methods. Finally, we demonstrate how the thematic information learnt from movie characters can potentially be used to understand themes in the experience of people, as indicated on Reddit posts.",
}

@inproceedings{wang-etal-2021-plot-twist,
   title={Plot Twist: Uncovering Surprising Event Boundaries in Narratives},
   author={Wang, Zhilin and Jafarpour, Anna and Sap, Maarten},
   year="2021",
   note={under review},
   pdf = "https://github.com/Zhilin123/Publications/blob/master/story_events.pdf",
   abs = {When reading stories, people can naturally identify sentences in which a new event starts, i.e., event boundaries, using their knowledge of how events typically unfold, but a computational model to detect event boundaries is not yet available.
          In this study, we characterize and detect sentences with expected or surprising event boundaries in an annotated corpus of short diary-like stories.
          We train a detection model that combines commonsense knowledge and narrative flow features with a RoBERTa classifier.
          Our results show that, while commonsense and narrative features can help improve performance overall, detecting event boundaries that are more subjective remains challenging for our model.
          Upon inspection of the model parameters, we find that sentences marking surprising event boundaries are less likely to be causally related to the preceding sentence, but are more likely to express emotional reactions of story characters, compared to sentences with no event boundary.
          Additionally, our model performs well in the closely related task of detecting stories with endings that do not follow commonsense from a different corpus.
          Our results show promise of using models with commonsense knowledge and narrative flow features to shed light on detecting event boundaries in narratives.
        },
}

@inproceedings{wang-etal-2019-youre,
    title = "No, you{'}re not alone: A better way to find people with similar experiences on {R}eddit",
    author = "Wang, Zhilin  and
      Rastorgueva, Elena  and
      Lin, Weizhe  and
      Wu, Xiaodong",
    booktitle = "Proceedings of the 5th Workshop on Noisy User-generated Text @ EMNLP",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5540",
    pdf = "https://aclanthology.org/D19-5540",
    doi = "10.18653/v1/D19-5540",
    pages = "307--315",
    abstract = "We present a probabilistic clustering algorithm that can help Reddit users to find posts that discuss experiences similar to their own. This model is built upon the BERT Next Sentence Prediction model and reduces the time complexity for clustering all posts in a corpus from O(n{\^{}}2) to O(n) with respect to the number of posts. We demonstrate that such probabilistic clustering can yield a performance better than baseline clustering methods based on Latent Dirichlet Allocation (Blei et al., 2003) and Word2Vec (Mikolov et al., 2013). Furthermore, there is a high degree of coherence between our probabilistic clustering and the exhaustive comparison O(n{\^{}}2) algorithm in which the similarity between every pair of posts is found. This makes the use of the BERT Next Sentence Prediction model more practical for unsupervised clustering tasks due to the high runtime overhead of each BERT computation.",
}
